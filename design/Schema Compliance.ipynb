{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe05ae2-3318-4fe4-bb7f-cff9ae83c3e2",
   "metadata": {},
   "source": [
    "# Dataframe Schema Compliance Notebook\n",
    "\n",
    "This Jupyter notebook demonstrates schema compliance validation for dataframes, focusing on data type consistency across Pandas and PyArrow frameworks. It explores different methods for writing and reading Parquet files, ensuring data types align with an expected schema.\n",
    "Schema\n",
    "## Overview\n",
    "The notebook defines an `expected_schema` for columns (`order_id`, `customer_id`, `revenue`, `order_time`) with PyArrow data types (e.g., `int64[pyarrow]`, `timestamp[ns, tz=UTC][pyarrow]`). It then tests schema compliance using a `generate_compliance_report` function, which compares actual data types against the schema and reports compliance percentages.\n",
    "\n",
    "## Key Sections\n",
    "1. **Pandas Write/Read (Default)** ❌ : Writes a Pandas dataframe to Parquet and reads it back without PyArrow backend, resulting in non-compliant types (e.g., `object` instead of `string` and `int64` instead of `int64[pyarrow]`), yielding 0% compliance.\n",
    "2. **Pandas Write/Read (PyArrow Backend)** ✅: Uses `dtype_backend='pyarrow'` when reading, achieving 100% compliance by preserving PyArrow types.\n",
    "3. **PyArrow Write/Read** ❌ : Creates a PyArrow table, saves it to Parquet, and converts it to Pandas without PyArrow backend, resulting in 0% compliance due to type mismatches.\n",
    "4. **PyArrow Write, Pandas Read (PyArrow Backend)** ✅: Reads the PyArrow-written Parquet file with `dtype_backend='pyarrow'`, ensuring 100% compliance.\n",
    "\n",
    "## Requirements\n",
    "- Python 3.10\n",
    "- Pandas, PyArrow\n",
    "\n",
    "This notebook highlights the importance of using the PyArrow backend for consistent data type handling across frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff5fa98-5506-49a7-bb28-4443ccb1e5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col:order_id, actual_dtype:object\n",
      "col:customer_id, actual_dtype:object\n",
      "col:revenue, actual_dtype:object\n",
      "col:order_time, actual_dtype:datetime64[ns, UTC]\n",
      "order_id: 66.67%\n",
      "customer_id: 66.67%\n",
      "revenue: 66.67%\n",
      "order_time: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pandas.api.types import is_integer_dtype, is_string_dtype, is_float_dtype\n",
    "\n",
    "# Define the expected schema\n",
    "expected_schema = {\n",
    "    \"order_id\": {\"data_type\": \"int64[pyarrow]\", \"nullable\": False},\n",
    "    \"customer_id\": {\"data_type\": \"string[pyarrow]\", \"nullable\": False},\n",
    "    \"revenue\": {\"data_type\": \"double[pyarrow]\", \"nullable\": False},\n",
    "    \"order_time\": {\"data_type\": \"timestamp[ns, tz=UTC][pyarrow]\", \"nullable\": False}\n",
    "}\n",
    "\n",
    "def generate_compliance_report(df, schema):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of rows in each column that have the correct data type and satisfy nullability.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to check.\n",
    "        schema (dict): The expected schema with data types and nullability.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A report mapping each column to its compliance percentage.\n",
    "    \"\"\"\n",
    "    report = {}\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    for col, spec in schema.items():\n",
    "        if col not in df.columns:\n",
    "            print(f\"col:{col}, actual_dtype:missing\")\n",
    "            report[col] = 0.0\n",
    "            continue\n",
    "        \n",
    "        expected_dtype = spec[\"data_type\"]\n",
    "        actual_dtype = str(df[col].dtype)\n",
    "        print(f\"col:{col}, actual_dtype:{actual_dtype}\")\n",
    "        \n",
    "        # Initialize compliance array (True for compliant rows)\n",
    "        compliance = pd.Series(True, index=df.index)\n",
    "        \n",
    "        # Check data type for each row\n",
    "        if expected_dtype == \"int64[pyarrow]\":\n",
    "            # Check if each value can be interpreted as an integer\n",
    "            compliance &= df[col].apply(lambda x: isinstance(x, (int, pd.Int64Dtype)) and not pd.isna(x))\n",
    "        elif expected_dtype == \"string[pyarrow]\":\n",
    "            # Check if each value is a string\n",
    "            compliance &= df[col].apply(lambda x: isinstance(x, str) and not pd.isna(x))\n",
    "        elif expected_dtype == \"double[pyarrow]\":\n",
    "            # Check if each value is a float\n",
    "            compliance &= df[col].apply(lambda x: isinstance(x, float) and not pd.isna(x))\n",
    "        elif expected_dtype == \"timestamp[ns, tz=UTC][pyarrow]\":\n",
    "            # Check if each value is a timestamp with UTC timezoneUntitled\n",
    "            compliance &= df[col].apply(\n",
    "                lambda x: pd.api.types.is_datetime64_any_dtype(pd.Series([x])) and\n",
    "                          hasattr(x, 'tz') and str(x.tz) == 'UTC' and\n",
    "                          not pd.isna(x)\n",
    "            )\n",
    "        \n",
    "        # Check nullability constraint\n",
    "        if not spec[\"nullable\"]:\n",
    "            compliance &= df[col].notnull()\n",
    "        \n",
    "        # Calculate percentage of compliant rows\n",
    "        compliant_rows = compliance.sum()\n",
    "        compliance_percentage = (compliant_rows / total_rows) * 100 if total_rows > 0 else 0.0\n",
    "        report[col] = compliance_percentage\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Sample dataset\n",
    "df = pd.DataFrame({\n",
    "    \"order_id\": [1, \"invalid\", 3],  # Second row has incorrect type\n",
    "    \"customer_id\": [\"A\", \"B\", 123],  # Third row has incorrect type\n",
    "    \"revenue\": [10.5, 20.0, \"invalid\"],  # Third row has incorrect type\n",
    "    \"order_time\": pd.date_range(\"2023-01-01\", periods=3, tz=\"UTC\")  # All correct\n",
    "})\n",
    "\n",
    "# Generate and print the compliance report\n",
    "report = generate_compliance_report(df, expected_schema)\n",
    "for col, compliance in report.items():\n",
    "    print(f\"{col}: {compliance:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088bbe1-18d1-4d86-986c-40d672fb9a4c",
   "metadata": {},
   "source": [
    "# however, we cannot store a column with mixed datatypes in parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbcb0318-8f34-40cd-a740-bb915ddc6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# Define the expected schema (unchanged)\n",
    "expected_schema = {\n",
    "    \"order_id\": {\"data_type\": \"int64[pyarrow]\", \"nullable\": False},\n",
    "    \"customer_id\": {\"data_type\": \"string[pyarrow]\", \"nullable\": False},\n",
    "    \"revenue\": {\"data_type\": \"double[pyarrow]\", \"nullable\": False},\n",
    "    \"order_time\": {\"data_type\": \"timestamp[ns, tz=UTC][pyarrow]\", \"nullable\": False}\n",
    "}\n",
    "\n",
    "# Assuming generate_compliance_report is defined elsewhere, generate and print the report\n",
    "# For demonstration, here's a placeholder if you don't have it:\n",
    "def generate_compliance_report(df, schema):\n",
    "    report = {}\n",
    "    for col, spec in schema.items():        \n",
    "        expected_dtype = spec[\"data_type\"]\n",
    "        actual_dtype = str(df[col].dtype)\n",
    "        print(f\"col:{col}, actual_dtype:{actual_dtype}\")\n",
    "        compliance = 100.0 if actual_dtype == expected_dtype else 0.0\n",
    "        report[col] = compliance\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016cf587-e65b-4efe-86b5-5f7226987f89",
   "metadata": {},
   "source": [
    "# pandas write parquet, pandas read (normal) ❌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02eda4d-280d-4eaa-8276-5df2f8a108e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col:order_id, actual_dtype:int64\n",
      "col:customer_id, actual_dtype:object\n",
      "col:revenue, actual_dtype:float64\n",
      "col:order_time, actual_dtype:datetime64[ns, UTC]\n",
      "order_id: 0.00%\n",
      "customer_id: 0.00%\n",
      "revenue: 0.00%\n",
      "order_time: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset (unchanged)\n",
    "df = pd.DataFrame({\n",
    "    \"order_id\": [1, 2, 3],\n",
    "    \"customer_id\": [\"A\", \"B\", None],\n",
    "    \"revenue\": [10.5, 20.0, 30.5],\n",
    "    \"order_time\": pd.date_range(\"2023-01-01\", periods=3, tz=\"UTC\")\n",
    "})\n",
    "df.to_parquet('pd_write.parquet')\n",
    "\n",
    "df=pd.read_parquet('pd_write.parquet')\n",
    "report = generate_compliance_report(df, expected_schema)\n",
    "for col, compliance in report.items():\n",
    "    print(f\"{col}: {compliance:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff1f80b-47a4-4f5e-9073-524198a3def2",
   "metadata": {},
   "source": [
    "# pandas write parquet, pandas read (dtype_backend='pyarrow') ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86527cc7-bdcf-4e54-b1b7-2204c6aca44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col:order_id, actual_dtype:int64[pyarrow]\n",
      "col:customer_id, actual_dtype:string[pyarrow]\n",
      "col:revenue, actual_dtype:double[pyarrow]\n",
      "col:order_time, actual_dtype:timestamp[ns, tz=UTC][pyarrow]\n",
      "order_id: 100.00%\n",
      "customer_id: 100.00%\n",
      "revenue: 100.00%\n",
      "order_time: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset (unchanged)\n",
    "df = pd.DataFrame({\n",
    "    \"order_id\": [1, 2, 3],\n",
    "    \"customer_id\": [\"A\", \"B\", None],\n",
    "    \"revenue\": [10.5, 20.0, 30.5],\n",
    "    \"order_time\": pd.date_range(\"2023-01-01\", periods=3, tz=\"UTC\")\n",
    "})\n",
    "df.to_parquet('pd_write.parquet')\n",
    "\n",
    "df=pd.read_parquet('pd_write.parquet',dtype_backend='pyarrow') #add this dtype_backend='pyarrow'\n",
    "report = generate_compliance_report(df, expected_schema)\n",
    "for col, compliance in report.items():\n",
    "    print(f\"{col}: {compliance:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581cb940-4f23-4ec2-aa0e-7a0e4ef7e3e8",
   "metadata": {},
   "source": [
    "# pyarrow write parquet, pyarrow read ❌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e2b1da-b3c7-4262-946d-5ff1e9e9b0b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyarrow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Step 3: Create the PyArrow table\u001b[39;00m\n\u001b[1;32m     23\u001b[0m table \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays([order_ids, customer_ids, revenues, order_times], schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[0;32m---> 24\u001b[0m [\u001b[43mpyarrow\u001b[49m]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Step 4: Save the table to a Parquet file\u001b[39;00m\n\u001b[1;32m     26\u001b[0m pq\u001b[38;5;241m.\u001b[39mwrite_table(table, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpa_write.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pyarrow' is not defined"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Create PyArrow arrays for each column\n",
    "order_ids = pa.array([1, 2, 3], type=pa.int64())\n",
    "customer_ids = pa.array([\"A\", \"B\", None], type=pa.string())\n",
    "revenues = pa.array([10.5, 20.0, 30.5], type=pa.float64())\n",
    "order_times = pa.array([pd.Timestamp(\"2023-01-01 12:00:00\", tz=\"UTC\"),\n",
    "                        pd.Timestamp(\"2023-01-02 12:00:00\", tz=\"UTC\"),\n",
    "                        pd.Timestamp(\"2023-01-03 12:00:00\", tz=\"UTC\")],\n",
    "                       type=pa.timestamp('ns', tz='UTC'))\n",
    "\n",
    "# Step 2: Define the schema\n",
    "schema = pa.schema([\n",
    "    pa.field(\"order_id\", pa.int64()),\n",
    "    pa.field(\"customer_id\", pa.string()),\n",
    "    pa.field(\"revenue\", pa.float64()),\n",
    "    pa.field(\"order_time\", pa.timestamp('ns', tz='UTC'))\n",
    "])\n",
    "\n",
    "# Step 3: Create the PyArrow table\n",
    "table = pa.Table.from_arrays([order_ids, customer_ids, revenues, order_times], schema=schema)\n",
    "[pyarrow]\n",
    "# Step 4: Save the table to a Parquet file\n",
    "pq.write_table(table, \"pa_write.parquet\")\n",
    "\n",
    "# Step 5: Load the Parquet file back into a PyArrow table\n",
    "table_loaded = pq.read_table(\"pa_write.parquet\")\n",
    "\n",
    "# Step 6: Convert the loaded table to a Pandas dataframe\n",
    "df = table_loaded.to_pandas()\n",
    "\n",
    "# Optional: Verify the dataframe by printing it and checking data types\n",
    "# print(\"Loaded Pandas DataFrame:\")\n",
    "# print(df)\n",
    "# print(\"\\nData types:\")\n",
    "# print(df.dtypes)\n",
    "\n",
    "report = generate_compliance_report(df, expected_schema)\n",
    "for col, compliance in report.items():\n",
    "    print(f\"{col}: {compliance:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956722c-2ed8-457b-8ec5-482df7e745ec",
   "metadata": {},
   "source": [
    "# pyarrow write parquet, pandas read ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef3bcba-da92-41ea-833d-27a5552ba576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('pa_write.parquet',dtype_backend='pyarrow')\n",
    "report = generate_compliance_report(df, expected_schema)\n",
    "for col, compliance in report.items():\n",
    "    print(f\"{col}: {compliance:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ab01e-1072-4141-82d9-59c123a4be57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
